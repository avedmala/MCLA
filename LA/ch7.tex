\chapter{Distance and Approximation}

\section{Inner Product Spaces}
We can define the dot product $u\cdot v$ of vectors $\vec{u}$ and $\vec{v}$ in $\mathbb{R}^n$.
An inner product on a vector space $V$ is an operation that assigns to every pair
of vectors $\vec{u}$ and $\vec{v}$ in $V$ a real number $\ev{u,v}$ such that the
following properties hold for all vectors $\vec{u}$, $\vec{v}$, and $\vec{w}$
in $V$ and all scalars $c$.

\subsection*{Properties of Inner Products}
Let $\vec{u}$, $\vec{v}$, and $\vec{w}$ be vectors in an inner product space
$V$ and let $c$ be a scalar.
\begin{enumerate}[(a)]
    \item $\ev{u+v,w}=\ev{u,w}$+$\ev{v,w}$
    \item $\ev{u,cv}=c\ev{uv}$
    \item $\ev{u,0}=\ev{0,v}$=0
\end{enumerate}

\subsection*{Definition}
Let $u$ and $v$ be vectors in an inner product space $V$
\begin{enumerate}
    \item The \textbf{length} of $v$ is $\Vert{V\Vert}=\sqrt{\ev{v,v}}$
    \item The \textbf{distance} between $u$ and $v$ is $d(u,v)=\Vert{u-v}\Vert$
    \item $u$ and $v$ are \textbf{orthogonal} if $\ev{u,v}=0$
\end{enumerate}

\subsection*{Example}
Consider the inner product on $\mathscr{L}[0,1]$ If $f(x)=x$ and $g(x)=3x-2$, find
\begin{enumerate}[(a)]
    \item $\Vert{f}\Vert$
    \item $d(f,g)$
    \item $\ev{f,g}$
\end{enumerate}

\subsection*{Solution}
\begin{enumerate}[(a)]
    \item \[\ev{f,f}=\int_0^1 f^2(x)\:dx=\int_0^1 x^2\:dx=\frac{x^3}{3}|^1_0=\frac{1}{3}\]
          \[\Vert{f}\Vert=\sqrt{\ev{f,f}}=\frac{1}{\sqrt{3}}\]
    \item \[d(f,g)=\Vert{f-g}\Vert=\sqrt{\ev{f-g,f-g}}\]
          \[f(x)-g(x)=x-(3x-2)=2-2x\]
          \[\ev{f-g,f-g}=\int_0^1 (f(x)-g(x))^2\:dx=4\left[x-x^2+\frac{x^3}{3}\right]_0^1=\frac{4}{3}\]
          \[d(f,g)=\sqrt{\frac{4}{3}}=\frac{2}{\sqrt{3}}\]
    \item \[\ev{f,g}=\int_0^1 f(x)g(x)\:dx=\int_0^1(3x^2-2x)\:dx=\left[x^3-x^2\right]_0^1=0\]
\end{enumerate}
Thus, $f$ and $g$ are orthogonal.

\subsection*{Pythagorasâ€™ Theorem}
Let $u$ and $v$ be vectors in an inner product space $V$. Then $u$ and $v$ are
orthogonal if and only if $\Vert{u + v}\Vert^2 = \Vert{u}\Vert^2 + \Vert{v}\Vert^2$

\subsection*{Example}
Construct an orthogonal basis for $\mathscr{P}_2$ with respect to the inner product
\[\ev{f,g}=\int_{-1}^1 f(x)g(x)\:dx\]
by applying the Gram-Schmidt Process to the basis $\{1,x,x^2\}$.

\subsection*{Solution}
Let $\*x_1=1$, $\*x_2=x$, and $\*x_3=x^2$. We begin by setting $\*v_1=x_1=1$. Next we compute
\[\ev{\*v_1,\*v_1}=\int_{-1}^1 dx=\left[x\right]_{-1}^1=2 \qquad \text{and} \qquad
    \ev{\*v_1,\*x_2}=\int_{-1}^1 x\:dx=\left[\frac{x^2}{2}\right]_{-1}^1=0\]
Therefore,
\[\*v_2=\*x_2-\frac{\ev{\*v_1,\*x_2}}{\ev{\*v_1,\*v_1}}\*v_1=x-\frac{0}{2}(1)=x\]
To find $\*v_3$, we first compute
\[\ev{\*v_1,\*x_3}=\int_{-1}^1x^2\:dx=\left[\frac{x^3}{3}\right]_{-1}^1=\frac{2}{3} \qquad
    \ev{\*v_2,\*x_3}=\int_{-1}^1x^3\:dx=\left[\frac{x^4}{4}\right]_{-1}^1=0\]
\[\ev{\*v_2,\*v_2}=\int_{-1}^1x^2\:dx=\frac{2}{3}\]
Then
\[
    \*v_3=\*x_3-\frac{\ev{\*v_1,\*x_3}}{\ev{\*v_1,\*v_1}}\*v_1-\frac{\ev{\*v_2,\*x_3}}{\ev{\*v_2,\*v_2}}\*v_2=x^2-\frac{\frac{2}{3}}{2}(1)-\frac{0}{\frac{2}{3}}x=x^2-\frac{1}{3}
\]
It follows that $\{\*v_1,\*v_2,\*v_3\}$ is an orthogonal basis for $\mathscr{P}_2$ on
the interval [-1, 1]. The polynomials
\[1,\quad x,\quad x^2-\frac{1}{3}\]
are the first three \textbf{Legendre polynomials}. If we divide each of these
polynomials byits length relative to the same inner product, we obtain
\textbf{normalized Legendre polynomials}.

\subsection*{The Cauchy-Schwarz Inequality}
Let $u$ and $v$ vectors in an inner product space $V$. Then
\[|\ev{u,v}|\leq\Vert{u}\Vert\Vert{v}\Vert\]
with equality holding if and only if $u$ and $v$ are scalar multiples of each other
\subsection*{The Triangle Inequality}
Let $u$ and $v$ vectors in an inner product space $V$. Then
\[\Vert{u+v}\Vert\leq\Vert{u}\Vert+\Vert{v}\Vert\]

\section{Norms and Distance Functions}
A norm on a vector space $V$ is a mapping that associates with each vector $\vec{v}$ a
real number $\Vert v\Vert$, called the \textbf{norm} of $\vec{v}$,
such that the following properties are satisfied for all vectors
$\vec{u}$ and $\vec{v}$ and all scalars $c$:
\begin{enumerate}
    \item $\Vert v\Vert\geq0$ and $\Vert v\Vert=0$ if and only if $v=0$
    \item $\Vert cv\Vert=|c|\Vert v\Vert$
    \item $\Vert u+v\Vert\leq\Vert u\Vert+\Vert v\Vert$
\end{enumerate}
A vector space with a norm is called a \textbf{}{normed linear space}.

\subsection*{Vector Norms}
\begin{enumerate}
    \item \textbf{Sum Norm} is the sum of the absolute values of its components
          \[\Vert v_s\Vert=|v_1|+\dots+|v_n|\]
    \item \textbf{Max Norm} is the largest number along the absolute values of its components
          \[\Vert v_m\Vert =\text{max}(|v_1|,\dots,|v_n|)\]
    \item \textbf{Euclidean Norm} is the value of the distance between the two vectors
\end{enumerate}

\subsection*{Example}
Let $u=\begin{bmatrix}
        3 \\-2
    \end{bmatrix}$ and $v=\begin{bmatrix}
        -1 \\1
    \end{bmatrix}$. Copmute $d(u,v)$ relative to (a) the Euclidean norm, (b)
the sum norm, and (c) the max norm.

\subsection*{Solution}
Each calculation requires knowing that $u-v=\begin{bmatrix}
        4 \\-3
    \end{bmatrix}$
\begin{enumerate}[(a)]
    \item $d_E(u,v)=\Vert u-v\Vert_E=\sqrt{4^2+(-3)^2}=\sqrt{25}=5$
    \item $d_S(u,v)=\Vert u-v\Vert_S=|4|+|-3|=7$
    \item $d_m(u,v)=\Vert u-v\Vert_m=\text{max}(|4|,|-3|)=4$
\end{enumerate}

\subsection*{Matrix Norms}
A Matrix Norm on $M_{nn}$ is a mapping that associates with each matrix $A$,
called the norm of $A$, and satisfies the following properties.
\begin{enumerate}
    \item $\Vert A\Vert\geq0$ and $\Vert A\Vert=0$ if and only if $A=0$
    \item $\Vert cA\Vert=|c|\Vert A\Vert$
    \item $\Vert A+B\Vert\leq\Vert A\Vert+\Vert B\Vert$
    \item $\Vert AB\Vert\leq\Vert A\Vert\Vert B\Vert$
\end{enumerate}

\section{Least Squares Approximation}
If $W$ is a subscape of a normed linear space $V$ and if $v$ is a vector in $V$,
then the best approximation to $v$ in $W$ such that
\[\Vert v-v \Vert < \Vert v-w \Vert\]
for every vector $w$ in $W$ different from $v$.

\subsection*{Least Squares Theorem}
Let $A$ be an $m\times n$ and let $b$ be in $\mathbb{R}^n$. Then
$A\*x = b$ always has at least one least squares solution. Moreover,
\begin{enumerate}
    \item $\bar{x}$ is a least square solution of $A\*x = b$ if and only if $\bar{x}$
          is a solution of the normal equations $A^TA\bar{x}=A^t\*b$
    \item $A$ has linearly independent coloumns if any only if $A^TA$ is invertible.
          In this case, the least squares solution is unique and is given by
          $\bar{x}=(A^TA)^{-1}A^T\*b$
\end{enumerate}

\subsection*{Penrose Conditions}
The \textbf{pseudoinverse} of $A$ is defined by $A^{+}=(A^TA)^{-1}A^T$. Note that
if $A$ is $m\times n$, then $A^+$ is $n\times m$. Then \textbf{Penrose Conditions} for $A$ are
\begin{enumerate}
    \item $AA^+A=A$
    \item $A^+AA^+=A^+$
    \item $AA^+$ and $A^+A$ are symmetric
\end{enumerate}

\section{The Singular Value Decomposition}
If $A$ is an $m\times n$, the \textbf{singular values} of $A$ are the square roots
of the eigenvalues of $A^TA$ and are denoted by $\sigma_1,\dots, \sigma_n$.

\subsection*{Singular Value Decomposition}
Let $A$ be an $m\times n$ with singular values $\sigma_1,\dots, \sigma_n$ and
$\sigma_{r+1}=\sigma_{r+2}=\dots=\sigma_{r+n}=0$. There exists an $m\times n$
orthogonal matrix $U$, an $n\times n$ orthogonal matrix $V$, and an $m\times n$ matrix
$\Sigma$ such that $A=U\Sigma V^T$.

\subsection*{Example}
Find a singular value deconomposition for the following matrix.
\[\begin{bmatrix}
        1 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}\]

\subsection*{Solution}
We compute
\[A^TA=\begin{bmatrix}
        1 & 1 & 0 \\
        1 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}\]
and find that its eigenvalues are $\lambda_1=2$, $\lambda_1=2$, and $\lambda_3=0$,
with corresponding eigenvectors
\[\begin{bmatrix}
        1 \\1\\0
    \end{bmatrix}\qquad\begin{bmatrix}
        0 \\0\\1
    \end{bmatrix}\qquad\begin{bmatrix}
        -1 \\1\\0
    \end{bmatrix}\]
These vectors are orthogonal, so we normalize them to obtain
\[\*v_1=\begin{bmatrix}
        1/\sqrt{2} \\
        1/\sqrt{2} \\
        {0}
    \end{bmatrix}\qquad\*v_2=\begin{bmatrix}
        0 \\
        0 \\
        1
    \end{bmatrix}\qquad\*v_3=\begin{bmatrix}
        -1/\sqrt{2} \\
        1/\sqrt{2}  \\
        {0}
    \end{bmatrix}\]
The singular values of $A$ are $\sigma_1=\sqrt{2}$, $\sigma_2=\sqrt{1}$, and
$\sigma_3=\sqrt{0}=0$. Thus,
\[V=\begin{bmatrix}
        1/\sqrt{2} & 0 & -1/\sqrt{2} \\
        1/\sqrt{2} & 0 & 1/\sqrt{2}  \\
        0          & 1 & 0
    \end{bmatrix}\qquad \text{and} \qquad \Sigma=\begin{bmatrix}
        \sqrt{2} & 0 & 0 \\
        0        & 1 & 0
    \end{bmatrix}\]
To find $U$, we compute
\[\*u_1=\frac{1}{\sigma_1}A\*v_1=\frac{1}{\sqrt{2}}\begin{bmatrix}
        1 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}\begin{bmatrix}
        1/\sqrt{2} \\
        1/\sqrt{2} \\
        {0}
    \end{bmatrix}=\begin{bmatrix}
        1 \\
        {0}
    \end{bmatrix}\]
\[\*u_2=\frac{1}{\sigma_2}A\*v_2=\frac{1}{1}\begin{bmatrix}
        1 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}\begin{bmatrix}
        0 \\
        0 \\
        1
    \end{bmatrix}=\begin{bmatrix}
        0 \\
        1
    \end{bmatrix}\]
These vectors already form an orthonormal basis for $\mathbb{R}^2$, so we have
\[U=\begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}\]
This yields the SVD
\[A=\begin{bmatrix}
        1 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}=\begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}\begin{bmatrix}
        \sqrt{2} & 0 & 0 \\
        0        & 1 & 0
    \end{bmatrix}\begin{bmatrix}
        1/\sqrt{2}  & 1/\sqrt{2} & 0 \\
        0           & 0          & 1 \\
        -1/\sqrt{2} & 1/\sqrt{2} & 0
    \end{bmatrix}=U\Sigma V^T\]

\subsection*{The Outer Product Form of the SVD}
\[a=\sigma_1u_1v_1^T+\dots+\sigma_ru_rv_r^T\]

\subsection*{Byproduct of Penrose Conditions}
$A^+=V\Sigma^+U^T$ where $\Sigma^+$ is the $n\times m$ matrix $\begin{bmatrix}
        D^{-1} & O \\
        O      & O
    \end{bmatrix}$

\subsection*{The Fundamental Theorem of Invertible Matricies}
\begin{enumerate}[(a)]
    \item $A$ is invertible.
    \item $A\*x=\*b$ has a unique solution for every $\*b$ in $\mathbb{R}^n$.
    \item $A\*x=\*0$ has only the trivial solution.
    \item The reduced row echelon form of $A$ is $I_n$.
    \item $A$ is the prodcut of elementary matrices.
    \item rank$(A)=n$
    \item nullity$(A)=0$
    \item The column vectors of $A$ are linearly independent.
    \item The column vectors of $A$ span $\mathbb{R}^n$.
    \item The column vectors of $A$ form a basis for $\mathbb{R}^n$.
    \item The row vectors of $A$ are linearly independent.
    \item The row vectors of $A$ span $\mathbb{R}^n$.
    \item The row vectors of $A$ form a basis for $\mathbb{R}^n$.
    \item det $A\neq0$
    \item 0 is not an eigenvalue of $A$.
    \item $T$ is invertible.
    \item $T$ is one-to-one.
    \item $T$ is onto.
    \item ker$(T)=0$
    \item range$(T)=W$
    \item 0 is not a singular value of $A$
\end{enumerate}

\section{Applications}

\subsection*{Example}
Find the best linear approximation to $f(x) = e^x$ on the interval $[-1, 1]$.

\subsection*{Solution}
\[\ev{f,g}=\int_{-1}^1 f(x)g(x)\:dx\]
A basis for $\mathscr{P}_1[-1,1]$ is given by $\{1,x\}$. Since
\[\ev{1,x}=\int_{-1}^1 x\:dx=0\]
this is an orthogonal basis, so the best approximation to $f$ in $W$ is
\begin{align*}
    g(x)=\text{proj}(e^x) & =\frac{\ev{1,e^x}}{\ev{1,1}}1+\frac{\ev{x,e^x}}{\ev{x,x}}x                                                       \\
                          & =\frac{\int_{-1}^1 (1\cdot e^x)\:dx}{\int_{-1}^1 (1\cdot1)\:dx}+\frac{\int_{-1}^1 xe^x\:dx}{\int_{-1}^1 x^2\:dx} \\
                          & =\frac{e-e^{-1}}{2}+\frac{2e^{-1}}{\frac{2}{3}}x                                                                 \\
                          & =\frac{1}{2}(e-e^{-1})+3e^{-1}x\approx1.18+1.10x
\end{align*}

\subsection*{Example}
Find the fourth-order Fourier approximation to $f(x) = x$ on $[-\pi, \pi]$

\subsection*{Solution}
\[
    a_0=\frac{1}{2\pi}\int_{-\pi}^\pi x\:dx=\frac{1}{2\pi}\left[\frac{x^2}{2}\right]_{-\pi}^\pi=0\\
\]
and for $k\geq1$, integration by parts yields
\[
    a_k=\frac{1}{\pi}\int_{-\pi}^\pi x\cos{kx}\:dx=\frac{1}{\pi}\left[\frac{x}{k}\sin{kx}+\frac{1}{k^2}\cos{kx}\right]_{-\pi}^\pi=0
\]
\begin{align*}
    b_k & =\frac{1}{\pi}\int_{-\pi}^\pi x\sin{kx}\:dx=\frac{1}{\pi}\left[-\frac{x}{k}\cos{kx}+\frac{1}{k^2}\sin{kx}\right]_{-\pi}^\pi \\
        & =\frac{1}{\pi}\left[\frac{-\pi\cos{k\pi}-\pi\cos{(-k\pi)}}{k}\right]                                                        \\
        & =\begin{cases}
        -\frac{2}{k} & \text{if } k \text{ is even} \\
        \frac{2}{k}  & \text{if } k \text{ is odd}
    \end{cases}                                                                                                 \\
        & =\frac{2(-1)^{k+1}}{k}
\end{align*}
It follows that the fourth-order Fourier approximation to $f(x) = x$ on $[-\pi, \pi]$ is
\[2\left(\sin{x}-\frac{1}{2}\sin{2x}+\frac{1}{3}\sin{3x}-\frac{1}{4}\sin{4x}\right)\]