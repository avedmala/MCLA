\chapter{Eigenvalues}

\section{Intro to Eigenvalues and Eigenvectors}

\subsection*{Definition}
Let $A$ be an $n\times n$ matrix. A scalar $\lambda$ is called an \textbf{eigenvalue}
of $A$ if there is a nonzero vector $\*x$ such that $A\*x=\lambda\*x$. Such a vector
$\*x$ is called an \textbf{eigenvector} of $A$ corresponding to $\lambda$.

\subsection*{Example}
Show that $\*x=\begin{bmatrix}
        1 \\1
    \end{bmatrix}$ is an eigenvector of $A=\begin{bmatrix}
        3 & 1 \\
        1 & 3
    \end{bmatrix}$ and find the corresponding eigenvalue.

\subsection*{Solution}
We compute
\[
    A\*x=\begin{bmatrix}
        3 & 1 \\
        1 & 3
    \end{bmatrix}\begin{bmatrix}
        1 \\1
    \end{bmatrix}=\begin{bmatrix}
        4 \\4
    \end{bmatrix}=4\begin{bmatrix}
        1 \\1
    \end{bmatrix}=4\*x
\]
from which it follows that $\*x$ is an eigenvector of $A$ corresponding to the eigenvalue 4.

\subsection*{Definition}
Let $A$ be an $n\times n$ matrix and let $\lambda$ be an eigenvalue of $A$. The collection
of all eigenvectors corresponding to $\lambda$, together with the zero vector, is called
the \textbf{eigenspace} of $\lambda$ and is denoted by $E_\lambda$.

\subsection*{Example}
Show that $\lambda=6$ is an eigenvalue of $A=\begin{bmatrix}
        7 & 1 & -2 \\-3&3&6\\2&2&2
    \end{bmatrix}$ and find a basis for its eigenspace.

\subsection*{Solution}
\[
    A-6I=\begin{bmatrix}
        1  & 1  & -2 \\
        -3 & -3 & 6  \\
        2  & 2  & -4
    \end{bmatrix}\arrows1{rref}\begin{bmatrix}
        1 & 1 & -2 \\
        0 & 0 & 0  \\
        0 & 0 & 0
    \end{bmatrix}
\]
from which we see that the null space of $A-6I$ is nonzero. Hence, 6 is an eigenvalue
of $A$, and the eigenvectors corresponding to this eigenvalue satisfy $x_1+x_2-2x_3=0$,
or $x_1=-x_2+2x_3$. It follows that
\[
    E_6=\left\{\begin{bmatrix}
        -x_2+2x_3 \\x_2\\x_3
    \end{bmatrix}\right\}=\left\{x_2\begin{bmatrix}
        -1 \\1\\0
    \end{bmatrix}+x_3\begin{bmatrix}
        2 \\0\\1
    \end{bmatrix}\right\}=\text{span}\left(\begin{bmatrix}
            -1 \\1\\0
        \end{bmatrix},\begin{bmatrix}
            2 \\0\\1
        \end{bmatrix}\right)
\]

\section{Determinants}

\subsection*{Definition}
Let $A=\begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33}
    \end{bmatrix}$. Then the \textbf{determinant} of $A$ is the scalar
\[
    \text{det }A=|A|=a_{11}\begin{vmatrix}
        a_{22} & a_{23} \\
        a_{32} & a_{33}
    \end{vmatrix}-a_{12}\begin{vmatrix}
        a_{21} & a_{23} \\
        a_{31} & a_{33}
    \end{vmatrix}+a_{13}\begin{vmatrix}
        a_{21} & a_{22} \\
        a_{31} & a_{32}
    \end{vmatrix}
\]

\subsection*{Example}
Compute the determinant of
\[A=\begin{bmatrix}
        5 & -3 & 2 \\
        1 & 0  & 2 \\
        2 & -1 & 3
    \end{bmatrix}\]

\subsection*{Solution}
We compute
\begin{align*}
    \text{det }A & =5\begin{vmatrix}
        0  & 2 \\
        -1 & 3
    \end{vmatrix}-(-3)\begin{vmatrix}
        1 & 2 \\
        2 & 3
    \end{vmatrix}+2\begin{vmatrix}
        1 & 0  \\
        2 & -1
    \end{vmatrix} \\
                 & =5(0-(-2))+3(3-4)+2(-1-0)                                                               \\
                 & =5(2)+3(-1)+2(-1)=5
\end{align*}

\subsection*{The Laplace Expansion Theorem}
The determinant of an $n\times n$ matrix $A=[a_{ij}]$, where $n\geq2$, can be computed as
\begin{align*}
    \text{det } & =a_{i1}C_{i1}+a_{i2}C_{i2}+\dots+a_{in}C_{in} \\
                & =\sum_{j=1}^n a_{ij}C_{ij}
\end{align*}
(which is the \textbf{cofactor expansion along the $i$th row}) and also as
\begin{align*}
    \text{det } & =a_{1j}C_{1j}+a_{2j}C_{2j}+\dots+a_{nj}C_{nj} \\
                & =\sum_{j=1}^n a_{ij}C_{ij}
\end{align*}
(the \textbf{cofactor expansion along the $j$the column}).

\subsection*{Example}
Compute the determinant of the matrix
\[A=\begin{bmatrix}
        5 & -3 & 2 \\
        1 & 0  & 2 \\
        2 & -1 & 3
    \end{bmatrix}\]
by (a) cofactor expansion along the third row and (b) cofactor expansion along the second column.

\subsection*{Solution}
\begin{enumerate}[(a)]
    \item We compute
          \begin{align*}
              \text{det } & =a_{31}C_{31}+a_{32}C_{32}+a_{33}C_{33}                                                 \\
                          & =2\begin{vmatrix}
                  -3 & 2 \\
                  0  & 2
              \end{vmatrix}-(-1)\begin{vmatrix}
                  5 & 2 \\
                  1 & 2
              \end{vmatrix}+3\begin{vmatrix}
                  5 & -3 \\
                  1 & 0
              \end{vmatrix} \\
                          & =2(-6)+8+3(3)                                                                           \\
                          & =5
          \end{align*}
    \item In this case, we have
          \begin{align*}
              \text{det } & =a_{12}C_{12}+a_{22}C_{22}+a_{32}C_{32}                                                     \\
                          & =-(-3)\begin{vmatrix}
                  1 & 2 \\
                  2 & 3
              \end{vmatrix}+0\begin{vmatrix}
                  5 & 2 \\
                  2 & 3
              \end{vmatrix}-(-1)\begin{vmatrix}
                  5 & 2 \\
                  1 & 2
              \end{vmatrix} \\
                          & =3(-1)+0+8                                                                                  \\
                          & =5
          \end{align*}
\end{enumerate}

\subsection*{Cramer's Rule}
Let $A$ be an invertible $n\times n$ matrix and let $\*b$ be a vector in $\mathbb{R}^n$.
Then the unique solution $\*x$ of the system $A\*x=\*b$ is given by
\[
    x_i=\frac{\text{det}(A_i(\*b))}{\text{det }A} \quad \text{for } i=1,\dots,n
\]

\subsection*{Example}
Use Cramer's Rule to solve the system
\[x_1+2x_2=2 \qquad -x_1+4x_2=1\]

\subsection*{Solution}
We compute
\[
    \text{det }A=\begin{vmatrix}
        1  & 2 \\
        -1 & 4
    \end{vmatrix}=6 \qquad
    \text{det}(A_1(\*b))=\begin{vmatrix}
        2 & 2 \\
        1 & 4
    \end{vmatrix}=6 \qquad
    \text{det}(A_2(\*b))=\begin{vmatrix}
        1  & 2 \\
        -1 & 1
    \end{vmatrix}=3
\]
By Cramer's Rule,
\[
    x_1=\frac{\text{det}(A_1(\*b))}{\text{det }A}=\frac{6}{6}=1 \qquad \text{and} \qquad
    x_2=\frac{\text{det}(A_2(\*b))}{\text{det }A}=\frac{3}{6}=\frac{1}{2}
\]
is called the \textbf{adjoint} of $A$ and is denoted by adj $A$.

\subsection*{Theorem}
Let $A$ be an invertible $n\times n$ matrix. Then
\[A^{-1}=\frac{1}{\text{det }A}\text{adj }A\]

\subsection*{Example}
Use the adjoint method to compute the inverse of
\[A=\begin{bmatrix}
        1 & 2 & -1 \\
        2 & 2 & 4  \\
        1 & 3 & -3
    \end{bmatrix}\]

\subsection*{Solution}
We compute det $A=-2$ and the nine cofactors
\[C_{11}=+\begin{vmatrix}
        2 & 4  \\
        3 & -3
    \end{vmatrix}=-18 \qquad C_{12}=-\begin{vmatrix}
        2 & 4  \\
        1 & -3
    \end{vmatrix}=10 \qquad C_{13}=+\begin{vmatrix}
        2 & 2 \\
        1 & 3
    \end{vmatrix}=4\]
\[C_{21}=-\begin{vmatrix}
        2 & -1 \\
        3 & -3
    \end{vmatrix}=3 \qquad C_{22}=+\begin{vmatrix}
        1 & -1 \\
        1 & -3
    \end{vmatrix}=-2 \qquad C_{23}=-\begin{vmatrix}
        1 & 2 \\
        1 & 3
    \end{vmatrix}=-1\]
\[C_{31}=+\begin{vmatrix}
        2 & -1 \\
        2 & 4
    \end{vmatrix}=10 \qquad C_{32}=-\begin{vmatrix}
        1 & -1 \\
        2 & 4
    \end{vmatrix}=-6 \qquad C_{33}=+\begin{vmatrix}
        1 & 2 \\
        2 & 2
    \end{vmatrix}=-2\]
THe adjoint is the \textbf{transpose} of the matrix of cofactors
\[\text{adj }A=\begin{bmatrix}
        -18 & 10 & 4  \\
        3   & -2 & -1 \\
        10  & -6 & -2
    \end{bmatrix}^T=\begin{bmatrix}
        -18 & 3  & 10 \\
        10  & -2 & -6 \\
        4   & -1 & -2
    \end{bmatrix}\]
Then
\[
    A^{-1}=\frac{1}{\text{det }A}\text{adj } A=-\frac{1}{2}\begin{bmatrix}
        -18 & 3  & 10 \\
        10  & -2 & -6 \\
        4   & -1 & -2
    \end{bmatrix}=\begin{bmatrix}
        9  & -3/2 & 5 \\
        -5 & 1    & 3 \\
        -2 & 1/2  & 1
    \end{bmatrix}
\]

\section{Eigenvalues and Eigenvectors of $n\times n$ Matrices}

The eigenvalues of a square matrix $A$ are precisely the solutions $\lambda$ of the equation
\[\text{det}(A-\lambda I)=0\]

\subsection*{Example}
Find the eigenvalues and the corresponding eigenspaces of
\[A=\begin{bmatrix}
        -1 & 0 & 1  \\
        3  & 0 & -3 \\
        1  & 0 & -1
    \end{bmatrix}\]

\subsection*{Solution}
The characteristic equation is
\begin{align*}
    0 & =\text{det}(A-\lambda I)=\begin{vmatrix}
        -1-\lambda & -        & 1          \\
        3          & -\lambda & -3         \\
        1          & 0        & -1-\lambda
    \end{vmatrix}=-\lambda\begin{vmatrix}
        -1-\lambda & 1          \\
        1          & -1-\lambda
    \end{vmatrix} \\
      & =-\lambda(\lambda^2+2\lambda)=-\lambda^2(\lambda+2)
\end{align*}

The eigenvalues are $\lambda_1=\lambda_2=0$ and $\lambda_3=-2$. The eigenvalue 0 has
algebriac multiplicity 2 and the eigenvalue -2 has algebriac multiplicity 1.

For $\lambda_1=\lambda_2=0$, we compute
\[[A-0I|0]=[A|0]=\left[\begin{array}{ccc|c}
            -1 & 0 & 1  & 0 \\
            3  & 0 & -3 & 0 \\
            1  & 0 & -1 & 0
        \end{array}\right]\to\left[\begin{array}{ccc|c}
            1 & 0 & 1 & 0 \\
            0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0
        \end{array}\right]\]
from which it follows that an eigenvector $\*x=\begin{bmatrix}
        x_1 \\x_2\\x_3
    \end{bmatrix}$ in $E_0$ satisfies $x_1=x_3$. Therefore, both $x_2$ and $x_3$ are free.
Setting $x_2=s$ and $x_3=t$, we have
\[
    E_0=\left\{\begin{bmatrix}
        t \\s\\t
    \end{bmatrix}\right\}=\left\{s\begin{bmatrix}
        0 \\1\\0
    \end{bmatrix}+t\begin{bmatrix}
        1 \\0\\1
    \end{bmatrix}\right\}=\text{span}\left(\begin{bmatrix}
            0 \\1\\0
        \end{bmatrix},\begin{bmatrix}
            1 \\0\\1
        \end{bmatrix}\right)
\]
For $\lambda_3=-2$,
\[[A-(-2)I|0]=[A+2I|0]=\left[\begin{array}{ccc|c}
            -1 & 0 & 1  & 0 \\
            3  & 0 & -3 & 0 \\
            1  & 0 & -1 & 0
        \end{array}\right]\to\left[\begin{array}{ccc|c}
            1 & 0 & 1 & 0 \\
            0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0
        \end{array}\right]\]
so $x_3=t$ is free and $x_1=-x_3=-t$ and $x_2=3x_3=3t$. Consequently,
\[
    E_{-2}=\left\{\begin{bmatrix}
        -t \\3t\\t
    \end{bmatrix}\right\}=\left\{t\begin{bmatrix}
        -1 \\3\\1
    \end{bmatrix}\right\}=\text{span}\left(\begin{bmatrix}
            -1 \\3\\1
        \end{bmatrix}\right)
\]
It follows that $\lambda_1=\lambda_2=0$ has geometric multiplicity 2 and $\lambda_3=-2$
has geometric multiplicity 1.

\subsection*{The Fundamental Theorem of Invertible Matrices}
Let $A$ be an $n\times n$ matrix. The following statements are equivalent:
\begin{enumerate}[(a)]
    \item $A$ is invertible.
    \item $A\*x=\*b$ has a unique solution for every $\*b$ in $\mathbb{R}^n$.
    \item $A\*x=\*0$ has only the trivial solution.
    \item The reduced row echelon form of $A$ is $I_n$.
    \item $A$ is the prodcut of elementary matrices.
    \item rank$(A)=n$
    \item nullity$(A)=0$
    \item The column vectors of $A$ are linearly independent.
    \item The column vectors of $A$ span $\mathbb{R}^n$.
    \item The column vectors of $A$ form a basis for $\mathbb{R}^n$.
    \item The row vectors of $A$ are linearly independent.
    \item The row vectors of $A$ span $\mathbb{R}^n$.
    \item The row vectors of $A$ form a basis for $\mathbb{R}^n$.
    \item det $A\neq0$
    \item 0 is not an eigenvalue of $A$.
\end{enumerate}

\section{Similarity and Diagonalization}

\subsection*{Definition}
Let $A$ and $B$ be $n\times n$ matrices. We say that $A$ is similar to $B$ if there
is an invertible $n\times n$ matrix $P$ such that $P^{-1}AP=B$. If $A$ is similar to $B$,
we write $A~B$.

\subsection*{Example}
Let $A=\begin{bmatrix}
        1 & 2  \\
        0 & -1
    \end{bmatrix}$ and $B=\begin{bmatrix}
        1  & 0  \\
        -2 & -1
    \end{bmatrix}$. Then $A~B$, since
\[
    \begin{bmatrix}
        1 & 2  \\
        0 & -1
    \end{bmatrix}\begin{bmatrix}
        1 & -1 \\
        1 & 1
    \end{bmatrix}=\begin{bmatrix}
        3  & 1  \\
        -1 & -1
    \end{bmatrix}=\begin{bmatrix}
        1 & -1 \\
        1 & 1
    \end{bmatrix}\begin{bmatrix}
        1  & 0  \\
        -2 & -1
    \end{bmatrix}
\]
Thus, $AP=PB$ with $P=\begin{bmatrix}
        1 & -1 \\
        1 & 1
    \end{bmatrix}$.

\subsection*{Definition}
An $n\times n$ matrix $A$ is \textbf{diagonalizable} if there is a diagonal matrix
$D$ such that $A$ is similar to $D$. That is, if there is an invertible $n\times n$ matrix
$P$ such that $P^{-1}AP=D$.

\subsection*{Example}
$A=\begin{bmatrix}
        1 & 3 \\
        2 & 2
    \end{bmatrix}$ is diagonalizable since, if $P=\begin{bmatrix}
        1 & 3  \\
        1 & -2
    \end{bmatrix}$ and $D=\begin{bmatrix}
        4 & 0  \\
        0 & -1
    \end{bmatrix}$, then $P^{-1}AP=D$, as can be easily checked.

\subsection*{The Diagonalization Theorem}
Let $A$ be an $n\times n$ matrix whose distinct eigenvalues are $\lambda_1,\lambda_2,\dots,\lambda_k$.
The following statements are equivalent:
\begin{enumerate}[(a)]
    \item $A$ is diagonalizable.
    \item The unique $\mathcal{B}$ of the bases of the eigenspaces of $A$ contains $n$ vectors.
    \item The algebriac multiplicity of each eigenvalue equals its geometric multiplicity.
\end{enumerate}

\section{Iterative Methods for Computing Eigenvalues}

\subsection*{Power Method}
Let $A$ be a diagonalizable $n\times n$ matrix with a corresponding dominant eigenvalue $\lambda_1$.
\begin{enumerate}
    \item Let $\*x_0=\*y_0$ be any initial vector in $\mathbb{R}^n$ whose largest component is 1.
    \item Repeat the following sets for $k=1,2,\dots :$
          \begin{enumerate}[(a)]
              \item Compute $\*x_k=A\*y_{k-1}$.
              \item Let $m_k$ be the component of $\*x_k$ with the largest absolute value.
              \item Set $\*y_k=(1/m_k)\*x_k$.
          \end{enumerate}
\end{enumerate}
For most choices of $\*x_0$, $m_k$ converges to the dominant eigenvalue $\lambda_1$ and
$\*y_k$ converges to a dominant eigenvector.

\subsection*{Example}
Use the power method to approximate the dominant eigenvalue and a dominant eigenvector of
\[A=\begin{bmatrix}
        0  & 5  & -6  \\
        -4 & 12 & -12 \\
        -2 & -2 & 10
    \end{bmatrix}\]

\subsection*{Solution}
Taking as our initial vector
\[\*x_0=\begin{bmatrix}
        1 \\1\\1
    \end{bmatrix}\]
You can see that the vectors $\*y_k$ are approaching $\begin{bmatrix}
        0.50 \\ 1 \\ -0.50
    \end{bmatrix}$ and the scalars $m_k$ are approaching 16. This suggests that they are,
respectively, a dominant eigenvector and the dominant eigenvalue of $A$.

\subsection*{Gerschgorin's Disk Theorem}
Let $A$ be an $n\times n$ matrix. Then every eigenvalue of $A$ is contained within a Gerschgorin disk.

\subsection*{Definition}
Let $A=[a_{ij}]$ be a $n\times n$ matrix, and let $r_i$ denote the sum of the absolute
values of the off-diagonal entires in the $i$th row of $A$; that is, $r_i=\sum_{j\neq i}|a_{ij}|$.
The \textbf{$i$th Gerschgorin disk} is the circular disk $D_i$ in the complex plane
with the center $a_{ij}$ and radius $r_i$. That is,
\[D=\{z\text{ in }\mathbb{C}:|z-a_{ij}|\leq r_i\}\]